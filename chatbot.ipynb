{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6L0Q5aVzQVAvmgDJRYlOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehul852/Mehul/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import libraries**"
      ],
      "metadata": {
        "id": "nGrTWVuXCWDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9-5ufx87Axn9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('chatbot.txt','r', errors='ignore')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower() #convert txts to lowercase\n",
        "nltk.download('punkt') #using the punkt tokenizer\n",
        "nltk.download('wordnet') #using wordnet dictionary\n",
        "sent_tokens=nltk.sent_tokenize(raw_doc) #converts doc to list of sentences\n",
        "word_tokens=nltk.word_tokenize(raw_doc) #converts doc to list of word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGMuxFiCfPx",
        "outputId": "0999f9aa-39aa-428d-e234-0d9f53d92276"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word tokens"
      ],
      "metadata": {
        "id": "nzAuxu3XH04_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens[ : 2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrrDXCqdHVE9",
        "outputId": "504c53d4-8dc0-4feb-e2aa-7271ef97bc99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['contents hide\\n(top)\\nfoundations\\ntoggle foundations subsection\\nrelationship to statistics\\netymology\\ntoggle etymology subsection\\nearly usage\\nmodern usage\\ndata science and data analysis\\ncloud computing for data science\\nethical consideration in data science\\nsee also\\nreferences\\ndata science\\n\\narticle\\ntalk\\nread\\nedit\\nview history\\n\\ntools\\nfrom wikipedia, the free encyclopedia\\nnot to be confused with information science.',\n",
              " 'the existence of comet neowise (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the wide-field infrared survey explorer.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text processing**"
      ],
      "metadata": {
        "id": "oetuV0QsIpeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def LemTokens(tokens):\n",
        "  return[lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict =dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "oAjXDRtCIeA2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining greeting funtions**"
      ],
      "metadata": {
        "id": "hdcZM2vpMuNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hey\", \"what's up\")\n",
        "GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\",\"hi there\",\"hello\",\"How may i assist you?\"]\n",
        "def greet(sentence):\n",
        "\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "metadata": {
        "id": "FDnfZoCTMtj_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**response genration**"
      ],
      "metadata": {
        "id": "p6kMCV2iNG8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "VtYzo7GcNGcg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words= 'english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx= vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"I'm sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response+sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "kckx1zdjQxcw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining conversation starts/end protocol**"
      ],
      "metadata": {
        "id": "jqoDOQhYTZ1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"BOT: My name is Mehul. Let's have a conversation! Also, if you want to exit, just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT: You are welcome..\")\n",
        "    else:\n",
        "      if(greet(user_response)!=None):\n",
        "        print(\"BOT: \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print(\"BOT: \",end=\"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT: Goodbye! Have a good day.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ0gRS_sTUzN",
        "outputId": "90dcc2ae-71a0-4532-eead-43036b668e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: My name is Mehul. Let's have a conversation! Also, if you want to exit, just type Bye!\n",
            "hi\n",
            "BOT: hi there\n",
            "watsup\n",
            "BOT: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry! I don't understand you\n",
            "what is data science?\n",
            "BOT: \"what is data science?\".\n",
            "Data Science\n",
            "BOT: \"what is data science?\".\n",
            "\"Ddata science\"\n",
            "BOT: science.\n",
            "\"what is data science?\"\n",
            "BOT: \"what is data science?\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "PStz1C7Ia2sG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pe3L2Wl7Q5Wm"
      }
    }
  ]
}